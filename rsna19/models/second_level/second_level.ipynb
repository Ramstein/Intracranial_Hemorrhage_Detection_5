{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm\n",
    "from rsna19.configs.second_level import Config\n",
    "from sklearn.metrics import log_loss\n",
    "import pandas as pd\n",
    "from scipy.signal import windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_folds = [0, 1, 2, 3] # for testing \n",
    "train_folds = [0, 1, 2, 3, 4]  # for stage 2 submission\n",
    "val_folds = [4]\n",
    "\n",
    "cache_dir = Config.cache_dir / '..'\n",
    "\n",
    "train_x = torch.cat([torch.tensor(np.load(f'{cache_dir}/fold{f}/x.npy'), dtype=torch.float32) for f in train_folds], dim=0)\n",
    "train_y = torch.cat([torch.tensor(np.load(f'{cache_dir}/fold{f}/y.npy'), dtype=torch.float32) for f in train_folds], dim=0)\n",
    "val_x = torch.cat([torch.tensor(np.load(f'{cache_dir}/fold{f}/x.npy'), dtype=torch.float32) for f in val_folds], dim=0)\n",
    "val_y = torch.cat([torch.tensor(np.load(f'{cache_dir}/fold{f}/y.npy'), dtype=torch.float32) for f in val_folds], dim=0)\n",
    "\n",
    "n_models = len(Config.models)\n",
    "class_weights = torch.tensor([1, 1, 1, 1, 1, 2], dtype=torch.float32) * 6 / 7\n",
    "loss_fn = F.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "model 0: 0.06121179461479187\n",
      "model 1: 0.06256157159805298\n",
      "model 2: 0.06102566421031952\n",
      "model 3: 0.0608997568488121\n",
      "model 4: 0.06243458017706871\n",
      "model 5: 0.0628940686583519\n",
      "model 6: 0.06294538080692291\n",
      "model 7: 0.06172546371817589\n",
      "model 8: 0.06102825328707695\n",
      "model 9: 0.06571821123361588\n",
      "model 10: 0.06511762738227844\n",
      "averaged ensemble: 0.056990209966897964\n",
      "\n",
      "val\n",
      "model 0: 0.061941273510456085\n",
      "model 1: 0.06328877061605453\n",
      "model 2: 0.0613483302295208\n",
      "model 3: 0.0610339418053627\n",
      "model 4: 0.06419891119003296\n",
      "model 5: 0.06281528621912003\n",
      "model 6: 0.06464578211307526\n",
      "model 7: 0.06388996541500092\n",
      "model 8: 0.06146755814552307\n",
      "model 9: 0.06970273703336716\n",
      "model 10: 0.06656711548566818\n",
      "averaged ensemble: 0.057923734188079834\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "preds = []\n",
    "for model_id in range(n_models):\n",
    "    preds.append(train_x[:, model_id*30+12:model_id*30+18])\n",
    "    loss = loss_fn(preds[-1], train_y, weight=class_weights)\n",
    "    print(f'model {model_id}: {loss}')\n",
    "\n",
    "mean_preds = torch.mean(torch.stack(preds), dim=0)\n",
    "loss = loss_fn(mean_preds, train_y, weight=class_weights)\n",
    "print(f'averaged ensemble: {loss}')\n",
    "\n",
    "print('\\nval')\n",
    "preds = []\n",
    "for model_id in range(n_models):\n",
    "    preds.append(val_x[:, model_id*30+12:model_id*30+18])\n",
    "    loss = loss_fn(preds[-1], val_y, weight=class_weights)\n",
    "    print(f'model {model_id}: {loss}')\n",
    "\n",
    "mean_preds = torch.mean(torch.stack(preds), dim=0)\n",
    "loss = loss_fn(mean_preds, val_y, weight=class_weights)\n",
    "print(f'averaged ensemble: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 330])\n",
      "0000: train: 0.1289, val: 0.1294\n",
      "0100: train: 0.1136, val: 0.1141\n",
      "0200: train: 0.0948, val: 0.0952\n",
      "0300: train: 0.0737, val: 0.0741\n",
      "0400: train: 0.0638, val: 0.0646\n",
      "0500: train: 0.0623, val: 0.0631\n",
      "0600: train: 0.0617, val: 0.0625\n",
      "0700: train: 0.0612, val: 0.0619\n",
      "0800: train: 0.0606, val: 0.0613\n",
      "0900: train: 0.0601, val: 0.0608\n",
      "1000: train: 0.0596, val: 0.0603\n",
      "1100: train: 0.0592, val: 0.0598\n",
      "1200: train: 0.0588, val: 0.0595\n",
      "1300: train: 0.0585, val: 0.0592\n",
      "1400: train: 0.0583, val: 0.0589\n",
      "1500: train: 0.0581, val: 0.0587\n",
      "1600: train: 0.0579, val: 0.0585\n",
      "1700: train: 0.0577, val: 0.0583\n",
      "1800: train: 0.0576, val: 0.0582\n",
      "1900: train: 0.0575, val: 0.0581\n",
      "2000: train: 0.0574, val: 0.0580\n",
      "2100: train: 0.0573, val: 0.0579\n",
      "2200: train: 0.0573, val: 0.0579\n",
      "2300: train: 0.0572, val: 0.0579\n",
      "2400: train: 0.0572, val: 0.0578\n",
      "2500: train: 0.0572, val: 0.0578\n",
      "2600: train: 0.0572, val: 0.0578\n",
      "2700: train: 0.0572, val: 0.0578\n",
      "2800: train: 0.0572, val: 0.0578\n",
      "2900: train: 0.0572, val: 0.0578\n",
      "3000: train: 0.0572, val: 0.0578\n",
      "3100: train: 0.0571, val: 0.0577\n",
      "3200: train: 0.0572, val: 0.0577\n",
      "3300: train: 0.0571, val: 0.0577\n",
      "3400: train: 0.0571, val: 0.0577\n",
      "3500: train: 0.0571, val: 0.0577\n",
      "3600: train: 0.0571, val: 0.0577\n",
      "3700: train: 0.0571, val: 0.0577\n",
      "3800: train: 0.0571, val: 0.0577\n",
      "3900: train: 0.0571, val: 0.0577\n",
      "4000: train: 0.0571, val: 0.0577\n",
      "4100: train: 0.0571, val: 0.0577\n",
      "4200: train: 0.0571, val: 0.0577\n",
      "4300: train: 0.0571, val: 0.0576\n",
      "4400: train: 0.0571, val: 0.0576\n",
      "4500: train: 0.0571, val: 0.0576\n",
      "4600: train: 0.0571, val: 0.0576\n",
      "4700: train: 0.0571, val: 0.0576\n",
      "4800: train: 0.0571, val: 0.0576\n",
      "4900: train: 0.0571, val: 0.0576\n",
      "5000: train: 0.0571, val: 0.0576\n",
      "5100: train: 0.0571, val: 0.0576\n",
      "5200: train: 0.0571, val: 0.0576\n",
      "5300: train: 0.0570, val: 0.0576\n",
      "5400: train: 0.0570, val: 0.0576\n",
      "5500: train: 0.0570, val: 0.0576\n",
      "5600: train: 0.0570, val: 0.0576\n",
      "5700: train: 0.0570, val: 0.0576\n",
      "5800: train: 0.0570, val: 0.0576\n",
      "5900: train: 0.0570, val: 0.0576\n",
      "6000: train: 0.0570, val: 0.0576\n",
      "6100: train: 0.0570, val: 0.0576\n",
      "6200: train: 0.0570, val: 0.0576\n",
      "6300: train: 0.0570, val: 0.0576\n",
      "6400: train: 0.0570, val: 0.0576\n",
      "6500: train: 0.0570, val: 0.0576\n",
      "6600: train: 0.0570, val: 0.0576\n",
      "6700: train: 0.0570, val: 0.0576\n",
      "6800: train: 0.0570, val: 0.0576\n",
      "6900: train: 0.0570, val: 0.0576\n",
      "7000: train: 0.0570, val: 0.0576\n",
      "7100: train: 0.0571, val: 0.0576\n",
      "7200: train: 0.0570, val: 0.0576\n",
      "7300: train: 0.0570, val: 0.0576\n",
      "7400: train: 0.0570, val: 0.0576\n",
      "7500: train: 0.0570, val: 0.0576\n",
      "7600: train: 0.0570, val: 0.0576\n",
      "7700: train: 0.0570, val: 0.0576\n",
      "7800: train: 0.0570, val: 0.0576\n",
      "7900: train: 0.0570, val: 0.0576\n",
      "8000: train: 0.0570, val: 0.0576\n",
      "8100: train: 0.0570, val: 0.0576\n",
      "8200: train: 0.0570, val: 0.0576\n",
      "8300: train: 0.0570, val: 0.0576\n",
      "8400: train: 0.0570, val: 0.0576\n",
      "8500: train: 0.0570, val: 0.0576\n",
      "8600: train: 0.0570, val: 0.0576\n",
      "8700: train: 0.0570, val: 0.0576\n",
      "8800: train: 0.0570, val: 0.0576\n",
      "8900: train: 0.0570, val: 0.0576\n",
      "9000: train: 0.0570, val: 0.0576\n",
      "9100: train: 0.0570, val: 0.0576\n",
      "9200: train: 0.0570, val: 0.0576\n",
      "9300: train: 0.0570, val: 0.0576\n",
      "9400: train: 0.0570, val: 0.0576\n",
      "9500: train: 0.0570, val: 0.0576\n",
      "9600: train: 0.0570, val: 0.0576\n",
      "9700: train: 0.0570, val: 0.0576\n",
      "9800: train: 0.0570, val: 0.0576\n",
      "9900: train: 0.0570, val: 0.0576\n"
     ]
    }
   ],
   "source": [
    "features_out = 6\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = torch.nn.Linear(train_x.shape[1], features_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, torch.abs(self.w1.weight) / torch.sum(torch.abs(self.w1.weight), 1, keepdim=True), self.w1.bias)\n",
    "        return torch.clamp(x, 0, 1)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "print(model.w1.weight.shape)\n",
    "\n",
    "train_x = train_x.cuda()\n",
    "train_y = train_y.cuda()\n",
    "val_x = val_x.cuda()\n",
    "val_y = val_y.cuda()\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.0001)\n",
    "val_log_loss = 0\n",
    "class_weights = class_weights.cuda()\n",
    "\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(train_x)\n",
    "    loss = F.binary_cross_entropy(y_hat, train_y, weight=class_weights)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        model.eval()\n",
    "        val_y_hat = model(val_x)\n",
    "        val_loss = F.binary_cross_entropy(val_y_hat, val_y, weight=class_weights)\n",
    "        model.train()\n",
    "        \n",
    "        print(f'{i:04d}: train: {loss.item():.04f}, val: {val_loss.item():.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.7581e-06, 3.8466e-06, 3.4292e-06,  ..., 3.0822e-06, 2.5072e-06,\n",
       "         5.4751e-06],\n",
       "        [2.1343e-06, 4.4915e-06, 4.5211e-06,  ..., 8.4711e-06, 1.2161e-05,\n",
       "         7.5307e-06],\n",
       "        [7.0367e-06, 1.1356e-05, 3.1004e-06,  ..., 6.7673e-06, 3.4150e-06,\n",
       "         4.6915e-06],\n",
       "        [1.2236e-05, 2.4873e-06, 5.0537e-06,  ..., 1.9682e-06, 9.5548e-06,\n",
       "         1.0263e-05],\n",
       "        [6.2068e-06, 8.9223e-06, 4.5145e-06,  ..., 6.6983e-06, 2.8349e-07,\n",
       "         5.9414e-06],\n",
       "        [9.9782e-06, 2.6110e-06, 1.6117e-05,  ..., 1.2043e-05, 1.2818e-07,\n",
       "         3.5978e-06]], device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(model.w1.weight) / torch.sum(torch.abs(model.w1.weight), 1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the trained model on the L1 test predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implement apply to test and call generate_submission with clip_eps 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test prediction of the same level 1 models and combine using trained l2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.mean([np.load(f'{cache_dir}/fold{f}/x_test.npy') for f in [0,1,2,3,4]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model(torch.tensor(test_x, dtype=torch.float32).cuda()).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/kolos/m2/ct/models/classification/rsna-ready2/0036_3x3_pretrained/fold0/predictions/test_rcrop+rrot+hflip.csv')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load one of the l1 model predictions as the base for l2 csv file:\n",
    "pred1_path = next((Config.models_root/Config.models[0]/'fold0/predictions').glob('test*'))\n",
    "pred1 = pd.read_csv(pred1_path)\n",
    "pred1_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = pred1.sort_values(by=['study_id', 'slice_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(Config.pred_columns):\n",
    "    pred1[c] = test_pred[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1.to_csv('pred_l2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
